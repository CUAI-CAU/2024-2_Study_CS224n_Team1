<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Lecture 2 - Word Vectors and Word Senses</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(249, 228, 188, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="10403738-5482-803b-b990-ea34833f86c8" class="page sans"><header><h1 class="page-title">Lecture 2 - Word Vectors and Word Senses</h1><p class="page-description"></p><table class="properties"><tbody><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesSelect"><path d="M8 15.126C11.8623 15.126 15.0615 11.9336 15.0615 8.06445C15.0615 4.20215 11.8623 1.00293 7.99316 1.00293C4.13086 1.00293 0.938477 4.20215 0.938477 8.06445C0.938477 11.9336 4.1377 15.126 8 15.126ZM8 13.7383C4.85547 13.7383 2.33301 11.209 2.33301 8.06445C2.33301 4.91992 4.84863 2.39746 7.99316 2.39746C11.1377 2.39746 13.6738 4.91992 13.6738 8.06445C13.6738 11.209 11.1445 13.7383 8 13.7383ZM7.62402 10.6348C7.79492 10.915 8.20508 10.9287 8.37598 10.6348L10.666 6.73145C10.8574 6.41016 10.7002 6.04102 10.3652 6.04102H5.62793C5.29297 6.04102 5.14941 6.43066 5.32031 6.73145L7.62402 10.6348Z"></path></svg></span>상태</th><td><span class="selected-value select-value-color-brown">Uploaded</span></td></tr><tr class="property-row property-row-created_time"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesCreatedAt"><path d="M8 15.126C11.8623 15.126 15.0615 11.9336 15.0615 8.06445C15.0615 4.20215 11.8623 1.00293 7.99316 1.00293C4.13086 1.00293 0.938477 4.20215 0.938477 8.06445C0.938477 11.9336 4.1377 15.126 8 15.126ZM8 13.7383C4.85547 13.7383 2.33301 11.209 2.33301 8.06445C2.33301 4.91992 4.84863 2.39746 7.99316 2.39746C11.1377 2.39746 13.6738 4.91992 13.6738 8.06445C13.6738 11.209 11.1445 13.7383 8 13.7383ZM4.54102 8.91211H7.99316C8.30078 8.91211 8.54004 8.67285 8.54004 8.37207V3.8877C8.54004 3.58691 8.30078 3.34766 7.99316 3.34766C7.69238 3.34766 7.45312 3.58691 7.45312 3.8877V7.83203H4.54102C4.2334 7.83203 4.00098 8.06445 4.00098 8.37207C4.00098 8.67285 4.2334 8.91211 4.54102 8.91211Z"></path></svg></span>생성 일시</th><td><time>@2024년 9월 17일 오후 6:31</time></td></tr><tr class="property-row property-row-last_edited_time"><th><span class="icon property-icon"><svg role="graphics-symbol" viewBox="0 0 16 16" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.45);flex-shrink:0" class="typesCreatedAt"><path d="M8 15.126C11.8623 15.126 15.0615 11.9336 15.0615 8.06445C15.0615 4.20215 11.8623 1.00293 7.99316 1.00293C4.13086 1.00293 0.938477 4.20215 0.938477 8.06445C0.938477 11.9336 4.1377 15.126 8 15.126ZM8 13.7383C4.85547 13.7383 2.33301 11.209 2.33301 8.06445C2.33301 4.91992 4.84863 2.39746 7.99316 2.39746C11.1377 2.39746 13.6738 4.91992 13.6738 8.06445C13.6738 11.209 11.1445 13.7383 8 13.7383ZM4.54102 8.91211H7.99316C8.30078 8.91211 8.54004 8.67285 8.54004 8.37207V3.8877C8.54004 3.58691 8.30078 3.34766 7.99316 3.34766C7.69238 3.34766 7.45312 3.58691 7.45312 3.8877V7.83203H4.54102C4.2334 7.83203 4.00098 8.06445 4.00098 8.37207C4.00098 8.67285 4.2334 8.91211 4.54102 8.91211Z"></path></svg></span>최종 편집 일시</th><td><time>@2024년 9월 18일 오후 9:19</time></td></tr></tbody></table></header><div class="page-body"><h3 id="10503738-5482-80ff-b107-c14098d2f800" class="block-color-yellow_background">Introduction and Recap</h3><ul id="8fa5861d-37ed-4934-8990-9553c976fb9d" class="bulleted-list"><li style="list-style-type:disc">단어를 벡터화 한 것은 단순히 유사도 계산을 용이하게 하는 것이 아니라, 실제 단어의 의미를 포착한다</li></ul><h3 id="10503738-5482-807d-99d3-cfba95d3d6ce" class="block-color-yellow_background">Word Vectors, Analogies, and Applications</h3><ul id="10503738-5482-8082-ac97-c80c302bce37" class="bulleted-list"><li style="list-style-type:disc"><code>analogy()</code> 함수로 의미의 덧셈 뺄셈 등이 가능<p id="561cf5d1-7d48-47f3-aa24-a5607d9dd731" class="">⇒ king → man = woman → ?<div class="indented"><p id="10503738-5482-80bf-bb35-f62894858600" class="">특정 벡터 (가장 유사한 단어 찾으면 queen)</p></div></p><p id="daeab06e-1987-49d7-80e2-d6478d194909" class="">⇒ autsralia → beer = france → ?<div class="indented"><p id="6d2802d1-9ce9-4b7d-9410-135434d73410" class="">특정 벡터 (가장 유사한 단어 찾으면 champagne)</p></div></p><p id="10503738-5482-80cd-985d-c6d389df6d75" class="">⇒ tall→ tallest= long→ ?<div class="indented"><p id="bc63aa55-9b44-4240-a905-a4d860cc7d8c" class="">특정 벡터 (가장 유사한 단어 찾으면 longest)</p></div></p><ul id="10503738-5482-80f2-9de0-ed246e9078d1" class="bulleted-list"><li style="list-style-type:circle">이렇게 하다보면 하나의 단어에 여러 의미가 묶이는 경우도 생김<ul id="10503738-5482-8033-8050-ddf0d36e0a51" class="bulleted-list"><li style="list-style-type:square">e.g.)  obama → clinton = reagan → ? 에서 nixon이 나옴<ul id="10503738-5482-8073-80a8-f20d21973e86" class="bulleted-list"><li style="list-style-type:disc">애초에 첫 비유가 모호했음</li></ul><ul id="10503738-5482-8066-a661-f1c97eb9adfc" class="bulleted-list"><li style="list-style-type:disc">강의자는 클린턴과 닉슨이 모두 탄핵 위기였다는 점을 듦</li></ul><ul id="10503738-5482-8075-8417-f83084a1b121" class="bulleted-list"><li style="list-style-type:disc">또한 클린턴이 빌 클린턴인지 힐러리 클린턴인지 알 수 없음. 단어에 둘 모두 묶임.</li></ul></li></ul></li></ul></li></ul><ul id="6fbb814e-18b7-42d5-90ac-4c1e0deca9c2" class="bulleted-list"><li style="list-style-type:disc"><code>doesnt_match()</code>함수를 이용해 여러 단어 중 다른 단어를 고를 수도 있음</li></ul><ul id="ab91b2c0-07eb-4eef-b2d7-b5339c7448f4" class="bulleted-list"><li style="list-style-type:disc">PCA를 통해 축소한 다음에 산포도를 그릴 수도 있음. <ul id="10503738-5482-80ed-b3f2-f9cddc875739" class="bulleted-list"><li style="list-style-type:circle">PCA는 100차원을 2차원으로 줄이다 보니 정보량이 상당히 많이 소실된다는 점을 조심해야</li></ul></li></ul><h3 id="10503738-5482-801c-ab5e-e619678a6a1c" class="block-color-yellow_background">Word2Vec and Contextual Predictions</h3><ul id="10503738-5482-8028-a1f6-c20a4cf3183d" class="bulleted-list"><li style="list-style-type:disc">Word2Vec 알고리즘<ul id="10503738-5482-80e7-aa0f-dc17c2f089fc" class="bulleted-list"><li style="list-style-type:circle">반복적 업데이트를 통해 단어의 벡터표현을 익히는 알고리즘</li></ul><ul id="10503738-5482-80bb-a486-c2c187e27b22" class="bulleted-list"><li style="list-style-type:circle">센터 단어를 중심으로 맥락 단어들을 추정하는 방식</li></ul></li></ul><ul id="c6340465-6dbc-414b-8527-3e7ab2837c43" class="bulleted-list"><li style="list-style-type:disc">Word2Vec 내 파라미터와 연산 구조<figure id="10503738-5482-8019-95b7-d44b3e01e465" class="image" style="text-align:center"><a href="image.png"><img style="width:432px" src="image.png"/></a></figure><ul id="5d2ad885-6887-4f52-b22b-ff34f3ff1a7e" class="bulleted-list"><li style="list-style-type:circle">U 안에 있는 각 단어는 행으로 표현됨, 맥락을 의미하는 V와 내적 후 소프트맥스 적용</li></ul><ul id="5f558947-5a3d-4489-a4a5-fb430414b035" class="bulleted-list"><li style="list-style-type:circle">‘that’, ‘of’, ‘and’ 등 자주 발생하는 말은 내적 값이 높을 것이다</li></ul></li></ul><ul id="10503738-5482-80b8-be3d-fd98fb20cf96" class="bulleted-list"><li style="list-style-type:disc">고차원 벡터 공간에서는 비직관적인 결과가 많이 나오기도<ul id="10503738-5482-8070-8eab-e614616cb936" class="bulleted-list"><li style="list-style-type:circle">어떠한 한 단어가 다른 여러 단어들과 서로 다른 여러 방향에서 가까울 수 있다</li></ul></li></ul><h3 id="4b6269d4-2e4e-4924-90a5-a471bfccb156" class="block-color-yellow_background">Negative Sampling and Optimization in Word2Vec</h3><ul id="10503738-5482-80c2-8375-deb68ee16638" class="bulleted-list"><li style="list-style-type:disc">최적화<ul id="10503738-5482-80e4-ad35-e3f69de38671" class="bulleted-list"><li style="list-style-type:circle">Gradient Descent<ul id="10503738-5482-8062-b28d-d464876eaffe" class="bulleted-list"><li style="list-style-type:square">갱신 함수<figure id="10503738-5482-8059-89a1-d7da8ce1e005" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>θ</mi><mtext>new</mtext></msub><mo>=</mo><msub><mi>θ</mi><mtext>old</mtext></msub><mo>−</mo><mi>α</mi><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\theta_{\text{new}} = \theta_{\text{old}} - \alpha \nabla_{\theta} J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">new</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">old</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></div></figure><ul id="10503738-5482-80cf-8e28-e1d9876eb79d" class="bulleted-list"><li style="list-style-type:disc">이때 <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span><span>﻿</span></span>는 <code>step size</code> 혹은 <code>learning rate</code>이라 한다</li></ul></li></ul><ul id="3a7c7070-5cd0-485b-aea2-8ed1d7af8075" class="bulleted-list"><li style="list-style-type:square">근데 이 손실함수는 corpus 내 모든 window에 대한 함수이므로 연산량이 엄청 많음</li></ul></li></ul><ul id="08b38612-2a68-4876-8f19-084f2f00eb18" class="bulleted-list"><li style="list-style-type:circle">Stochastic Gradient Descent<ul id="10503738-5482-8067-9a3f-fd142af85cc0" class="bulleted-list"><li style="list-style-type:square">window를 샘플링하여 연산량을 줄임 → 그래디언트를 추정한 후 파라미터 업데이트에 활용함</li></ul><ul id="10503738-5482-806e-abdf-f62b143eb662" class="bulleted-list"><li style="list-style-type:square">굉장히 noisy한 추정이지만 별로 중요하지 않음<p id="10503738-5482-80fa-b0e6-e1520f659a89" class="">실제로는? 32나 64 차원짜리 샘플을 따옴, 이 미니배치의 그래디언트를 추정함</p><p id="10503738-5482-8090-a135-c5453b995c64" class="">→ 미니배치를 활용하여 평균했으므로 노이즈 감소, 병렬화 가능하므로 gpu 활용 可</p></li></ul></li></ul><ul id="5210ca01-9726-4cdb-b336-f38792cfbccc" class="bulleted-list"><li style="list-style-type:circle">워드 벡터를 활용한 SGD<figure id="10503738-5482-807a-9c38-d78de0c172b9" class="image" style="text-align:center"><a href="image%201.png"><img style="width:288px" src="image%201.png"/></a></figure><ul id="10503738-5482-8027-9b60-ff65afbfff46" class="bulleted-list"><li style="list-style-type:square">비전 딥러닝 등과 다름</li></ul><ul id="10503738-5482-8061-8707-cee5ccc592b3" class="bulleted-list"><li style="list-style-type:square">미니배치는 많아야 100~150개의 단어를 가지고 있을 것 (32짜리 배치, 윈도우 크기 10)</li></ul><ul id="10503738-5482-800b-a408-e0144b123f2a" class="bulleted-list"><li style="list-style-type:square">근데 총 어휘집은 단어 25만 개 정도</li></ul><ul id="10503738-5482-8064-8752-ff87d9aac40c" class="bulleted-list"><li style="list-style-type:square">따라서 대부분의 벡터 내 대부분의 원소의 값은 0에 근접함</li></ul><ul id="10503738-5482-8012-9755-f04000a0dbae" class="bulleted-list"><li style="list-style-type:square">이러한 희소행렬 문제를 해결하려면, 실제로 등장하는 워드 벡터만을 업데이트 해야함.</li></ul><ul id="10503738-5482-80ec-9ed9-e4ee42ea88ee" class="bulleted-list"><li style="list-style-type:square">즉, 특정 행만을 업데이트 하는 행렬 연산이 있어야 함. 또는 해시테이블 가지거나</li></ul><ul id="10503738-5482-80b5-9d8a-c37bf9d4bc0a" class="bulleted-list"><li style="list-style-type:square">두개의 벡터를 쓰는 것이 현실적으로 최적화하기 편함.<ul id="10503738-5482-8024-9f6b-f5302582e178" class="bulleted-list"><li style="list-style-type:disc">하나의 벡터만을 가지고 연산하면 center word기 두 번 나와서 제곱 나옴 →계산 복잡해짐</li></ul></li></ul></li></ul><ul id="ae375d12-d257-4d99-8080-45343825fb8b" class="bulleted-list"><li style="list-style-type:circle">Skip-Gram 모델 : center 단어를 통해 context 단어를 예측</li></ul><ul id="10503738-5482-808d-be68-c50822544273" class="bulleted-list"><li style="list-style-type:circle">Continuous Bag of Words 모델 : context 단어를 통해 center 단어를 예측</li></ul><ul id="10503738-5482-8012-af9e-f895a460f197" class="bulleted-list"><li style="list-style-type:circle">이후는 과제 설명이긴 한데, 시그모이드 함수가 소프트맥스 함수의 binary version 이라는 점이 인상적</li></ul></li></ul><ul id="7f73b0cd-814d-4104-89dd-ccde818187dc" class="bulleted-list"><li style="list-style-type:disc">Negative Sampling:<figure id="10503738-5482-80b0-8f59-cff997c5b055" class="image"><a href="image%202.png"><img style="width:336px" src="image%202.png"/></a></figure><ul id="10503738-5482-800e-9a8a-fb5c71a5b56f" class="bulleted-list"><li style="list-style-type:circle">정규화가 보통 너무 무거운 작업이라 word2vec 에서는 <code>Negative Sampling</code>을 활용한 <code>skip-gram</code> 모델이 구현됨</li></ul></li></ul><p id="10503738-5482-80cf-b203-e036a18fe47e" class="">
</p><h3 id="10503738-5482-806e-bed5-f7e7155639a8" class="block-color-yellow_background">Count-Based Methods, SVD, and GloVe</h3><ul id="10503738-5482-8012-b2a8-f2973ce2b0ff" class="bulleted-list"><li style="list-style-type:disc">Neural Networks 등장 전에는 단어 의미를 co-occurrence 행렬을 통해 포착</li></ul><ul id="10503738-5482-80ee-abda-fdcd01f069c9" class="bulleted-list"><li style="list-style-type:disc"><strong>Singular Value Decomposition (SVD)</strong>는 co-occurrence 행렬에서 차원 축소를 통해 단어 간 유사성을 포착</li></ul><ul id="10503738-5482-80dc-8970-d137fd6e5f17" class="bulleted-list"><li style="list-style-type:disc"><strong>GloVe</strong>는 count-based와 prediction-based 방법을 결합한 모델<ul id="10503738-5482-805d-864b-d644b42f8a5e" class="bulleted-list"><li style="list-style-type:circle"><strong>Log of co-occurrence probabilities</strong>를 사용해 단어 벡터 공간에서 선형적 관계를 표현</li></ul><ul id="10503738-5482-80fe-803b-c66d264e1378" class="bulleted-list"><li style="list-style-type:circle">벡터 간 선형 관계는 유추 문제에서 중요한 역할을 함</li></ul><ul id="c9d62f51-f09a-4bf5-9b24-e6931c1c2e88" class="bulleted-list"><li style="list-style-type:circle">GloVe는 효율적인 학습을 위해 말뭉치 전체를 활용하고 계산 복잡도를 줄임</li></ul></li></ul><h3 id="10503738-5482-80ad-9e64-c735f69b1682" class="block-color-yellow_background">Evaluations and Word Sense Ambiguity</h3><ul id="10503738-5482-80e1-a2b0-c7ee194b1207" class="bulleted-list"><li style="list-style-type:disc">Word Vectors의 성능은 <strong>intrinsic</strong>(유추 문제)와 <strong>extrinsic</strong>(실제 과제 성능) 평가로 나뉨</li></ul><ul id="10503738-5482-80a9-ab0a-f5854bbd4df3" class="bulleted-list"><li style="list-style-type:disc"><strong>Cosine similarity</strong>로 단어 벡터 간 유사성을 측정</li></ul><ul id="10503738-5482-803c-9bc9-fa4af1fd3b7c" class="bulleted-list"><li style="list-style-type:disc"><strong>Word sense ambiguity</strong>(단어 의미 모호성) 문제: 한 벡터가 여러 의미를 혼합해 표현함<ul id="10503738-5482-801e-92d3-d20d8ba59263" class="bulleted-list"><li style="list-style-type:circle">예: &quot;Jaguar&quot;는 자동차, 동물 등 여러 의미를 가짐</li></ul></li></ul><ul id="10503738-5482-80cd-a715-e47739bc6ba3" class="bulleted-list"><li style="list-style-type:disc"><strong>Superposition Principle</strong>: 단어 벡터는 여러 의미의 가중 평균으로 나타남<ul id="10503738-5482-806b-b313-cf29b0168267" class="bulleted-list"><li style="list-style-type:circle">희소한 의미 벡터를 sparse coding 기법으로 복원 가능</li></ul><ul id="10503738-5482-807a-ac4b-fffcc71ab8a5" class="bulleted-list"><li style="list-style-type:circle">이를 통해 여러 가지 의미를 분리할 수 있음</li></ul></li></ul><h3 id="10503738-5482-8062-a9ac-c15cfa5e244f" class="block-color-yellow_background">Word Vectors in NLP and Conclusion</h3><ul id="10503738-5482-8004-b6d1-dc66c8f7c4ce" class="bulleted-list"><li style="list-style-type:disc">Word Vectors는 Named Entity Recognition, Text Classification 등 다양한 NLP 과제에서 성능을 향상시킴</li></ul><ul id="10503738-5482-809a-ad15-ff852d708a5a" class="bulleted-list"><li style="list-style-type:disc">Pre-trained word vectors는 다양한 NLP 시스템에서 재사용 가능</li></ul><ul id="10503738-5482-804b-9ada-dd48eb3ba5d6" class="bulleted-list"><li style="list-style-type:disc">Wikipedia와 같은 백과사전형 텍스트는 더 좋은 성능을 제공</li></ul><ul id="10503738-5482-809e-9f45-ec8b3ba17d89" class="bulleted-list"><li style="list-style-type:disc">말뭉치의 크기와 품질은 단어 벡터 성능에 큰 영향을 미침</li></ul><ul id="682f71ad-32dc-46aa-89ad-738addace3c9" class="bulleted-list"><li style="list-style-type:disc">Word Vectors는 NLP에서 중요한 기술로 자리 잡음<ul id="7d0d8f5e-9912-4065-acf3-a09eac3fd1c3" class="bulleted-list"><li style="list-style-type:circle">고차원 벡터는 다양한 응용에서 성능을 크게 향상시킴</li></ul></li></ul></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>